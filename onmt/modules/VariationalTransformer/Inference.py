import numpy as np
import torch, math
import torch.nn as nn
import torch.nn.functional as F
from onmt.modules.Transformer.Layers import PositionalEncoding
from onmt.modules.Transformer.Layers import EncoderLayer, DecoderLayer
from onmt.modules.StochasticTransformer.Layers import StochasticEncoderLayer, StochasticDecoderLayer
from onmt.modules.Transformer.Models import TransformerEncoder, TransformerDecoder
import onmt
from onmt.modules.WordDrop import embedded_dropout
from onmt.modules.Transformer.Layers import XavierLinear, MultiHeadAttention, FeedForward, PrePostProcessing
Linear = XavierLinear

import copy
"""
    Variational Inference for model depth generation

    Our model structure is generated by a latent variable z = {z_1, z_2 .... z_n} corresponding to n layers
    Assumption is each layer is generated randomly (motivated by the Stochastic Network)
    Mean Field assumption is used (one set of parameters for each z)

    Our loss function is:

    L = E_q_z ( log (p (Y|X, z)) - KL( q(z|X, y) || p(z|X))
    (data likelihood given the latent variable)
    
    The Prior model estimates p(z | x)

    The Posterior model estimates q(z | x, y)

    During training we take the sample from posterior (variational inference)
    During testing  y is not available, so we use the prior (conditional prior)

"""

def mean_with_mask(context, mask):

    # context dimensions: T x B x H
    # mask dimension: T x B x 1 (with unsqueeze)
    # first, we have to mask the context with zeros at the unwanted position

    eps = 0
    context.masked_fill_(mask, eps)

    # then take the sum over the time dimension
    context_sum = torch.sum(context, dim=0, keepdim=False)


    weights = torch.sum(1 - mask, dim=0, keepdim=False).type_as(context_sum)

    mean = context_sum.div_(weights)

    return mean

"""
    The Prior model estimates p(z | x)
"""
class NeuralPrior(nn.Module):

    """Encoder in 'Attention is all you need'
    
    Args:
        opt: list of options ( see train.py )
        dicts : dictionary (for source language)
        
    """
    def __init__(self, opt, embedding, positional_encoder):
    
        super(NeuralPrior, self).__init__()

        encoder_opt = copy.deepcopy(opt)
        # quick_hack to override some hyper parameters of the prior encoder
        # encoder_opt.layers = 
        #  encoder_opt.word_dropout = 0.0 
        self.dropout = opt.dropout

        self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)

        self.projector = Linear(opt.model_size, opt.model_size)
        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)

    def forward(self, input, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """
        # pass the input to the transformer encoder (we also return the mask)
        context, _ = self.encoder(input, freeze_embedding=True)
          
        # Now we have to mask the context with zeros
        # context size: T x B x H
        # mask size: T x B x 1 for broadcasting
        mask = input.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        context = mean_with_mask(context, mask)
        encoder_meaning = context
        context = F.tanh(self.projector(context))
       

        mean = self.mean_predictor(context)       
        var = torch.nn.functional.softplus(self.var_predictor(context))

        p_z = torch.distributions.normal.Normal(mean.float(), var.float())

        

        # return prior distribution P(z | X)
        return encoder_meaning, p_z


class NeuralPosterior(nn.Module):

    """Neural Posterior using Transformer
    
    Args:
        opt: list of options ( see train.py )
        embedding : dictionary (for target language)
        
    """
    
    def __init__(self, opt, embedding, positional_encoder):
    
        super(NeuralPosterior, self).__init__()
        
        encoder_opt = copy.deepcopy(opt)

        # quick_hack to override some hyper parameters of the prior encoder
        # encoder_opt.layers = 4
        # encoder_opt.word_dropout = 0.0 
        self.dropout = opt.dropout
        self.projector = Linear(opt.model_size * 2, opt.model_size)
        # self.encoder_src = TransformerEncoder(encoder_opt, dicts_src, positional_encoder)
        self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)
        # self.norm = nn.LayerNorm(opt.model_size)

        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)
    
    def forward(self, encoder_meaning, input_src, input_tgt, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """

        """ Embedding: batch_size x len_src x d_model """
        
        # encoder_context = encoder_context.detach()
        decoder_context, _ = self.encoder(input_tgt, freeze_embedding=True)

        # src_mask = input_src.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)
        tgt_mask = input_tgt.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)


        # take the mean of each context
        # encoder_context = mean_with_mask(encoder_context, src_mask)

        encoder_context = encoder_meaning
        decoder_context = mean_with_mask(decoder_context, tgt_mask)

        context = torch.cat([encoder_context, decoder_context], dim=-1)

        context = F.tanh(self.projector(context))

        mean = self.mean_predictor(context)
        var = torch.nn.functional.softplus(self.var_predictor(context))

        q_z = torch.distributions.normal.Normal(mean.float(), var.float())


        # return distribution Q(z | X, Y)
        return q_z
