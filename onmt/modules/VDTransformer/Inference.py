import numpy as np
import torch, math
import torch.nn as nn
import torch.nn.functional as F
from onmt.modules.Transformer.Layers import PositionalEncoding
from onmt.modules.Transformer.Layers import EncoderLayer, DecoderLayer
from onmt.modules.StochasticTransformer.Layers import StochasticEncoderLayer, StochasticDecoderLayer
from onmt.modules.Transformer.Models import TransformerEncoder, TransformerDecoder
import onmt
from onmt.modules.WordDrop import embedded_dropout
from onmt.modules.Transformer.Layers import XavierLinear, MultiHeadAttention, FeedForward, PrePostProcessing
Linear = XavierLinear

import copy
"""
	Variational Inference for model depth generation

	Our model structure is generated by a latent variable z = {z_1, z_2 .... z_n} corresponding to n layers
	Assumption is each layer is generated randomly (motivated by the Stochastic Network)
	Mean Field assumption is used (one set of parameters for each z)

	Our loss function is:

	L = E_q_z ( log (p (Y|X, z)) - KL( q(z|X, y) || p(z|X))
	(data likelihood given the latent variable)
	
	The Prior model estimates p(z | x)

	The Posterior model estimates q(z | x, y)

	During training we take the sample from posterior (variational inference)
	During testing  y is not available, so we use the prior (conditional prior)

"""


"""
	The Prior model estimates p(z | x)


"""
class NeuralPrior(nn.Module):

	"""Encoder in 'Attention is all you need'
    
    Args:
        opt: list of options ( see train.py )
        dicts : dictionary (for source language)
        
    """
    
    def __init__(self, opt, dicts, positional_encoder):
    
        super(NeuralPrior, self).__init__()
        
        # self.model_size = opt.model_size
        # self.n_heads = opt.n_heads
        # self.inner_size = opt.inner_size
        # self.layers = 2
        # self.dropout = opt.dropout
        # self.word_dropout = opt.word_dropout
        # self.attn_dropout = opt.attn_dropout
        # self.emb_dropout = opt.emb_dropout
        # self.time = opt.time
        # self.num_z = opt.layers
        
        # self.word_lut = nn.Embedding(dicts.size(),
        #                              self.model_size,
        #                              padding_idx=onmt.Constants.PAD)
        
        # self.time_transformer = positional_encoder
        
        # self.preprocess_layer = PrePostProcessing(self.model_size, self.emb_dropout, sequence='d', static=False)
        
        # self.postprocess_layer = PrePostProcessing(self.model_size, 0, sequence='n')
        
        # self.positional_encoder = positional_encoder

        encoder_opt = copy.deep_copy(opt)
        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = 2
        encoder_opt.word_dropout = 0.0 

        self.encoder = TransformerEncoder(encoder_opt, dicts, positional_encoder)

        self.predictor = nn.Linear(self.model_size, self.num_z)

    def forward(self, input, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """
        # pass the input to the transformer encoder
        context = self.encoder(input, kwargs)
          
        # Now we have to mask the context with zeros
        # context size: T x B x H
        mask = input.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        context.masked_fill_(mask, 0)

        # take the mean pooling
        mean = torch.mean(context, dim=0, keep_dim=False)
        # batch_size x H

        # Bernoulli distribution
        # size: batch_size x n_layers (0 1 for each layer)
        p_z = torch.sigmoid(self.predictor(mean))

        return p_z


class NeuralPosterior(nn.Module):

	"""Encoder in 'Attention is all you need'
    
    Args:
        opt: list of options ( see train.py )
        dicts : dictionary (for source language)
        
    """
    
    def __init__(self, opt, dicts, positional_encoder):
    
        super(NeuralPosterior, self).__init__()
        
        encoder_opt = copy.deep_copy(opt)

        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = 2
        encoder_opt.word_dropout = 0.0 


        self.predictor = nn.Linear(self.model_size, self.num_z)
    
        self.build_modules()
        
    def build_modules(self):
        
        self.layer_modules = nn.ModuleList([EncoderLayer(self.n_heads, self.model_size, self.dropout, self.inner_size, self.attn_dropout) for _ in range(self.layers)])

    def forward(self, input_src, input_tgt, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """

        """ Embedding: batch_size x len_src x d_model """
        # emb = embedded_dropout(self.word_lut, input, dropout=self.word_dropout if self.training else 0)
        emb = self.word_lut(input)
        
        """ Scale the emb by sqrt(d_model) """
        
        emb = emb * math.sqrt(self.model_size)
            
        """ Adding positional encoding """
        emb = self.time_transformer(emb)
        
        emb = self.preprocess_layer(emb)
        
        mask_src = input.eq(onmt.Constants.PAD).unsqueeze(1) # batch_size x 1 x len_src for broadcasting
        
        #~ pad_mask = input.ne(onmt.Constants.PAD)) # batch_size x len_src
        
        context = emb.transpose(0, 1).contiguous()
        
        for i, layer in enumerate(self.layer_modules):
            
            if len(self.layer_modules) - i <= onmt.Constants.checkpointing and self.training:        
                context = checkpoint(custom_layer(layer), context, mask_src)

            else:
                context = layer(context, mask_src)      # batch_size x len_src x d_model
            
        
        # From Google T2T
        # if normalization is done in layer_preprocess, then it should also be done
        # on the output, since the output can grow very large, being the sum of
        # a whole stack of unnormalized layer outputs.    
        context = self.postprocess_layer(context)
          
        # Now we have to mask the context with zeros
        # context size: T x B x H
        mask = input.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        context.masked_fill_(mask, 0)

        # take the mean pooling
        mean = torch.mean(context, dim=0, keep_dim=False)
        # batch_size x H

        # Bernoulli distribution
        # size: batch_size x n_layers (0 1 for each layer)
        p_z = torch.sigmoid(self.predictor(mean))

        return p_z